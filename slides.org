#+TITLE:     ODE parameter estimation in drug research: a deep dive into a Bayesian inference engine
#+AUTHOR:    Yi Zhang
#+EMAIL:     yz@yizh.org
#+DATE:      <2023-05-26 Fri>
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS:   H:2 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:https://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+HTML_LINK_UP:
#+HTML_LINK_HOME:

#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{xcolor}

#+startup: beamer
#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [bigger]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_ACT(Act) %10BEAMER_extra(Extra)
#+BEAMER_HEADER: \AtBeginSection[]{\begin{frame}<beamer>\frametitle{Table of contents}\tableofcontents[currentsection]\end{frame}}

** Thanks
*** Another model                                                     :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
#+ATTR_LATEX: :width 0.8\textwidth
[[./figure/sxplogo.png]]
#+ATTR_LATEX: :width 0.8\textwidth
[[./figure/onr_logo.png]]

*** Another model                                                     :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:

#+ATTR_LATEX: :width 0.3\textwidth
[[./figure/stan_logo.png]]

- Stan development team (Bob Carpenter, Daniel Lee, Sebastian Weber, ...)
- Bill Gillespie (Metrum Research Group)



** NOT to talk about
*** dont want 1                                                       :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.33
:END:
#+ATTR_LATEX: :width 0.9\textwidth
[[./figure/dont_want_2.jpg]]


*** dont want 2                                                       :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.33
:END:
#+ATTR_LATEX: :width 0.9\textwidth
[[./figure/dont_want_3.jpg]]

*** dont want 3                                                       :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.33
:END:
Inference engine/Probabilistic programing language(PPL)
- +Language design+
- Inference algorithms
- +Model checking & diagnostics+
- +Model assessment and comparison+


* The problem
** Pharmacokinetics (PK) data
*** Data
:PROPERTIES:
:BEAMER_col: 0.50
:END:
#+CAPTION: Subject plasma concentration history (q12hx14).
#+ATTR_LATEX: :width \textwidth
[[./figure/twocpt_ppc_4x8.pdf]]

*** 2cpt
:PROPERTIES:
:BEAMER_col: 0.50
:END:
#+ATTR_LATEX: :width \textwidth
[[./figure/TwoCptNice.png]]

** A Pharmacokinetics (PK) model
\begin{align*}
  \frac{\mathrm d \hat{z}_\mathrm{gut}}{\mathrm d t} & = - k_a \hat{z}_\mathrm{gut} \\
  \frac{\mathrm d \hat{z}_\mathrm{cent}}{\mathrm d t} & = k_a \hat{z}_\mathrm{gut} -\left (\frac{\text{CL}}{V_{\text{cent}}} + \frac{Q}{V_{\text{cent}}} \right) \hat{z}_\mathrm{cent} + \frac{Q}{V_{\text{peri}}} \hat{z}_\mathrm{peri} \\
  \frac{\mathrm d \hat{z}_\mathrm{peri}}{\mathrm d t} & = \frac{Q}{V_{\text{cent}}} \hat{z}_\mathrm{cent} - \frac{Q}{V_{\text{peri}}} \hat{z}_\mathrm{peri},\\
\end{align*}
With $\hat{y} = \frac{\hat{z}}{V_{\text{cent}}}$:
\begin{align*}
\theta & \equiv \{k_a, \text{CL}, Q, V_{\text{cent}}, V_{\text{peri}}, \dots\},\\
\hat{y}(t) & = \hat{y}(t;\theta),\\
y(t) &\sim \text{Normal}(\hat{y}(t;\theta), \sigma).
\end{align*}

** A statistical model
- play mix & match with two models
- Likelihood $p(y | \theta)$
  \begin{align*}
    p(y| \theta) &=\frac{1}{\sqrt{2\pi\sigma^2}}\exp{-\frac{(\hat{y}-y)^2}{2\sigma^2}},\\
    \log p(y|\theta) &=-\frac{(\hat{y}-y)^2}{2\sigma^2}-\log{2\pi\sigma^2}
  \end{align*}
- Usually $\sigma$ is also unknow so likelihood $p(y | \theta, \sigma)$


** Estimate $\theta$
- <1->  Maximum likelihood estimation
  $\theta_{\text{argmax}}(p(y|\theta))$
- <2-> Posterior distribution
  \begin{align*}
    p(\theta | y) &= \frac{p(y|\theta)p(\theta)}{\bigcirc}\\
    \log{p(\theta|y)} &= C + \log{p(y | \theta)} + \log{p(\theta)}
  \end{align*}
- <3-> Connection to regularization: $\log{p(y | \theta)}= -\frac{(\hat{y}-y)^2}{2\sigma^2}$
  \begin{align*}
    \min{\|\hat{y}(\theta)-y\|^2 + \lambda\|\theta\|^2},\\
    \log p(\theta|y)=-\frac{(\hat{y}-y)^2}{2\sigma^2} + \log p(\theta)
  \end{align*}
  "Tikhonov regularization", "ridge regression", "shrinkage"

** Prior & likelihood
\begin{align*}
  \log{p(\theta|y)} &= C + \log{p(y | \theta)} + \log{p(\theta)}
\end{align*}
#+ATTR_LATEX: :width 0.7\textwidth
[[./figure/prior_and_data.png]]


** Sample the posterior
$\text{target (distribution)} = \log{p(\theta|y)} = \log{p(y | \theta)} + \log{p(\theta)}$

- <1-> Posterior from an oracle: $\theta \rightarrow \log{p(\theta | y)}$
- <2-> Every query costs one ODE solution
- <3-> Sampling procedure for a distribution: $\theta^{(1)}, \theta^{(2)}, \dots$
  \begin{equation*}
    \lim_{n\rightarrow }\frac{1}{n}\sum{f(\theta^{(i)})}=\mathbb{E}(f)
  \end{equation*}
- <4-> MCMC: $p(\theta^{(i)} | y, \theta^{(i-1)},  \theta^{(i-2)}, \dots, \theta^{(1)})=p(\theta^{(i)} | y, \theta^{(i-1)})$


** Posterior samples: trace
:PROPERTIES:
:END:
#+ATTR_LATEX: :width \textwidth
[[./figure/twoCpt_trace.pdf]]

** Posterior samples: density
:PROPERTIES:
:END:
#+ATTR_LATEX: :width \textwidth
[[./figure/twoCpt_dens.pdf]]


** The Metropolis-Hastings algorithm
Proposal-rejection from $\theta^{(i)}$ to $\theta^{(i+1)}$.
1. Propose $\theta^{(i+1)}$ according to transition density $q(\theta^{(i+1)}, \theta^{(i)})$.
2. Accept $\theta^{(i+1)}$ with probability
\begin{equation}
  \alpha(\theta^{(i)}, \theta^{(i+1)}) = \min\left[
    1, \frac{p(\theta^{(i+1)}|y)q(\theta^{(i)}, \theta^{(i+1)})}{p(\theta^{(i)}|y)q(\theta^{(i+1)}, \theta^{(i)})}
    \right]
\end{equation}
   otherwise reject.

- Chain generated by M-H has detailed balance with $p(\theta|y)$ as
  its stationary distribution.
- Convergence in TVD with proper proposal that garantees irreducibility and aperiodicity.

** Why MCMC?
#+BEGIN_QUOTE
Monte Carlo is an extremely bad method; it should be used only when all alternative methods
are worse.
-- Sokal, A. D. (1989). "Monte carlo methods in statistical mechanics: foundations and new algorithms."
#+END_QUOTE
#+ATTR_LATEX: :width 0.7\textwidth
[[./figure/simply_not_mcmc.jpg]]


* You're Gonna Need a Bigger Boat
** The problem
#+ATTR_LATEX: :width \textwidth
[[./figure/twocpt_pop_ppc1.pdf]]

** Hierarchical model
*** A model                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
\begin{align*}
\theta_0 &\sim \text{Prior}(\cdot),\\
\theta_i &\sim \text{MultiNormal}(\theta_0, \Sigma),\\
y_i &\sim \text{Normal}(\hat{y}_i(\theta_i), \sigma),\\
\theta &= \{\theta_0, \theta_1, \dots, \theta_n, \Sigma\}
\end{align*}

*** Another model                                                     :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
- <1-> Posterior(likelihood) equation is an oracle
- <2-> Curse of dimensionality
  + <2-> Computational tractability
  + <3-> Concentration of measure (e.g. high dimensional gaussian distribution is like uniform distribution)
- <4-> Geometry of posterior

** Challenges: high dimensional gaussian distribution
$p(|\|y_d\|_2-\sqrt{d}| \ge t) \le 2\exp{(-ct^2)}, \forall t\ge 0$.
*** Another model                                                     :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
  # purrr::map_dfr(c(2, 10, 100), ~MASS::mvrnorm(n = 1000, mu=rep(0, .x), #
  #       				       Sigma=diag(1, .x, .x)) |> as_tibble(.name_repair =
  #       									       janitor::make_clean_names) |> rename(x_1=x) |>
  #       			    dplyr::mutate(across(starts_with('x_'), ~.x * .x,
  #       						 .names="square_{.col}"), .keep="unused") |> rowwise() |>
  #       			    mutate(y=sum(c_across(starts_with("square"))), L2=sqrt(y), n=.x,
  #       				   .keep="unused")) |> ggplot(aes(L2, color=factor(n))) + geom_density()
  # ggsave("figure/high_dim_gaussian.png")

#+caption: $\|y_d\|_2, y_d \sim \text{Normal}(0, \mathbb{I}_d)$
#+ATTR_LATEX: :width 0.8\textwidth
[[./figure/high_dim_gaussian.png]]

Average is not representative.
Random Walk sampler is not efficient.

*** Another model                                                     :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
# MASS::mvrnorm(n = 1000, mu=rep(0, 1), Sigma=diag(1, 1, 1)) |> as_tibble() |> ggplot(aes(V1,V2))+geom_point()
#+ATTR_LATEX: :width 0.6\textwidth
[[./figure/d2_normal_point.png]]

# tibble(theta=rep(seq(0, 2*pi, by=0.05), 5), r=rnorm(length(theta), 10, 0.5)) |> ggplot(aes(theta, r)) + geom_point(alpha=0.4) + coord_polar() + ylim(0,12) + scale_x_continuous(breaks = 1:6)
#+ATTR_LATEX: :width 0.6\textwidth
[[./figure/d100_normal_point.png]]

** Challenges: Geometry of posterior
*** Another model                                                     :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
\begin{align*}
  \theta_0 &= 0,\\
  \kappa &\sim \text{Normal}(0, 3),\\
  \theta_i(k) &\sim \text{Normal}(0, \exp{(\kappa/2)}),\\
  k&=1,2,\dots
\end{align*}

*** Another model                                                     :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
#+caption: Neal's funnel
#+ATTR_LATEX: :width \textwidth
[[./figure/funnel.png]]
Mode is not representative.
Optimizer is not efficient.

* Up In the Air
** Hamiltonian Monte Carlo
\begin{align*}
  H(\theta, r) &= -\log{p(r, \theta | y)} = T(r) + V(\theta) = -\log{r} - \log{p(\theta|y)},\\
  \frac{d\theta}{dt} &= \frac{\partial H}{\partial r},\qquad
  \frac{dr}{dt} = -\frac{\partial H}{\partial \theta},
\end{align*}
*** Another model                                                     :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
# MASS::mvrnorm(n = 1000, mu=rep(0, 1), Sigma=diag(1, 1, 1)) |> as_tibble() |> ggplot(aes(V1,V2))+geom_point()
#+ATTR_LATEX: :width 0.8\textwidth
[[./figure/d2_normal_point.png]]

*** Another model                                                     :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
# MASS::mvrnorm(n = 1000, mu=rep(0, 1), Sigma=diag(1, 1, 1)) |> as_tibble() |> ggplot(aes(V1,V2))+geom_point()
#+ATTR_LATEX: :width 0.8\textwidth
[[./figure/d100_normal_point.png]]


** Hamiltonian Monte Carlo
\begin{align*}
  H(\theta, r) &= -\log{p(r, \theta | y)} = T(r) + V(\theta) = -\log{r} - \log{p(\theta|y)},\\
  \frac{d\theta}{dt} &= \frac{\partial H}{\partial r},\qquad
  \frac{dr}{dt} = -\frac{\partial H}{\partial \theta},
\end{align*}
Apply M-H to $p(r, \theta)$
\begin{align*}
  \alpha((r^{(i)}, \theta^{(i)}), (r^{(i+1)}, \theta^{(i+1)})) &= \min\left[
    1, \frac{p(r^{(i+1)}, \theta^{(i+1)}|y)q()}{p(r^{(i)}, \theta^{(i)}|y)q()}
  \right]\\
  &= \min\left[
    1, \frac{p(r^{(i+1)}, \theta^{(i+1)}|y)}{p(r^{(i)}, \theta^{(i)}|y)}
  \right]
\end{align*}
\begin{equation*}
  \alpha(\cdot, \cdot) =\min\left[
    1, \exp{(H(\theta^{(i)}, r^{(i)}) - H(\theta^{(i+1)}, r^{(i+1)}))}
  \right]
\end{equation*}


** A principled sampler for $p(\theta|y)$
\begin{align*}
  p(\theta^{(i)} \rightarrow \theta^{(i+1)}) =\min\left[
    1, \exp{(H(\theta^{(i)}, r^{(i)}) - H(\theta^{(i+1)}, r^{(i+1)}))}
  \right]\\
  r \sim \text{Normal}(0, M) \Longrightarrow
  (r^{(i)}, \theta^{(i)}) \rightarrow (r^{(i+1)}, \theta^{(i+1)})
\end{align*}
# dat <- bind_rows(
#   tibble(theta1 = rnorm(7000, sd = 1),
#          theta2 = rnorm(7000, sd = 10),
#          group = "foo"),
#   tibble(theta1 = rnorm(3000, mean = 1, sd = .5),
#          theta2 = rnorm(3000, mean = 7, sd = 5),
#          group = "bar"))
# dat |> ggplot(aes(theta1, theta2)) + geom_density_2d_filled() + xlim(-2,2.5) + ylim(-20,20) + geom_curve(data=d2, aes(x1=theta1, y1=theta2, xend=theta1.end, yend=theta2.end), colour="white", arrow = arrow(length = unit(0.1, "inches"))) + geom_point(data=d2, aes(theta1, theta2), colour="white") + coord_fixed( ratio=1) + theme(legend.position = "none")
*** Another model                                                     :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.7
:END:
#+ATTR_LATEX: :width 0.7\textwidth
[[./figure/sampler_path.png]]

*** Another model                                                     :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.2
:END:
#+ATTR_LATEX: :width 0.5\textwidth
[[./figure/sampler_path2.png]]


* Small moves, Ellie. Small moves
** A tale of two ODEs
\theta^{(i)} = {\theta^{(i)}_j}, j=1,2,\dots,n: $(r, \theta)(\tau^{(i)}) \rightarrow (r, \theta)(\tau^{(i+1)})$
\begin{align*}
\begin{cases}
  &\theta^{(i)}_j \rightarrow \hat{y}_j(t; \theta^{(i)}_j),\\
  &y_{jk} \sim \text{Normal}(\hat{y}_{jk}(\theta_j), \sigma),\quad p(y_{jk}|\theta_j) = \frac{1}{\sigma\sqrt{2\pi}}\exp{\left[-\frac{(\hat{y}_{jk}(\theta_j)-y_{jk})^2}{2\sigma^2}\right]}
\end{cases}
\end{align*}
Nested ODE solvers:
\begin{align*}
  r^{(i+1/2)} &= r^{(i)} - \frac{h}{2}\nabla_{\theta} \log{p(\theta^{(i)} | y)},\text{  a step in leapfrog}\\
  \nabla_{\theta} \log{p(\theta^{(i)} | y)} &= \nabla_{\theta} \log{p(\theta^{(i)})} + \nabla_{\theta} \log{p(y | \theta^{(i)})},\\
  \nabla_{\theta} \log{p(y | \theta^{(i)})} &= - (\cdots)\sum_{j,k}\nabla_{\theta} \frac{\hat{y}_{jk}(\theta_j^{(i)}) - y_{jk}}{\sigma^{(i)}} + \dots
\end{align*}

** Automatic differentiation (1 obsv/subject: k=1)
#+ATTR_LATEX: :width \textwidth
[[./figure/autodiff_0_diag.pdf]]

** Automatic differentiation (1 obsv/subject: k=1)
#+ATTR_LATEX: :width \textwidth
[[./figure/autodiff_diag.pdf]]


** Automatic differentiation
*** Another model                                                     :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
#+ATTR_LATEX: :width 0.7\textwidth
[[./figure/stan_single_funcs.png]]

*** Another model                                                     :BMCOL:
:PROPERTIES:
:BEAMER_act: <2->
:BEAMER_col: 0.5
:END:
#+ATTR_LATEX: :width 0.9\textwidth
[[./figure/everyone_adjoint.jpg]]

** Sensitivity solution
  # \nabla_{\theta} \hat{y}_{jk}&: ?

\begin{align*}
  &\frac{d\hat{y}}{dt} = f(t, \hat{y};\theta) \rightarrow \nabla_{\theta} \frac{d\hat{y}}{dt} = \nabla_{\theta} f(t, \hat{y};\theta)\\
  &\frac{d\nabla_{\theta} \hat{y}}{dt} = f_{\theta} + f_{\hat{y}}\nabla_{\theta}\hat{y}
\end{align*}
Use the autodiff calculate $f_{\theta}$ and $f_{\hat{y}}$.


** Thanks
